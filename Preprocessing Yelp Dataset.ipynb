{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10214867,"sourceType":"datasetVersion","datasetId":6313718}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom pathlib import Path","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T12:21:28.790095Z","iopub.execute_input":"2025-11-19T12:21:28.790793Z","iopub.status.idle":"2025-11-19T12:21:28.799609Z","shell.execute_reply.started":"2025-11-19T12:21:28.790757Z","shell.execute_reply":"2025-11-19T12:21:28.798340Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!ls /kaggle/input/processed-yelp-dataset/data/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T12:21:42.225016Z","iopub.execute_input":"2025-11-19T12:21:42.225406Z","iopub.status.idle":"2025-11-19T12:21:42.363264Z","shell.execute_reply.started":"2025-11-19T12:21:42.225377Z","shell.execute_reply":"2025-11-19T12:21:42.361993Z"}},"outputs":[{"name":"stdout","text":"business_data.csv  review_testing.csv  review_training.csv  users_data.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Fonction de nettoyage des données\ndef clean_data(df):\n    # Supprimer les doublons selon user, business et note\n    df = df.drop_duplicates(subset=[\"user_id\", \"business_id\", \"stars\"])\n    \n    # Supprimer les lignes avec valeurs manquantes importantes\n    df = df.dropna(subset=[\"user_id\", \"business_id\", \"stars\", \"text\"])\n    \n    # Forcer le type int pour les notes\n    df[\"stars\"] = df[\"stars\"].astype(int)\n    \n    return df\n\n# Fonction de traitement par batch (chunks) CSV → Parquet\ndef process_csv_in_chunks(csv_path, parquet_path, chunksize=100000):\n    writer = None  # Écrivain Parquet\n    for chunk in pd.read_csv(\n        csv_path,\n        chunksize=chunksize,\n        usecols=[\"review_id\", \"user_id\", \"business_id\", \"stars\", \"text\" , \"date\"]\n    ):\n        # Nettoyage du batch\n        clean_chunk = clean_data(chunk)\n\n        # Conversion en table Apache Arrow\n        table = pa.Table.from_pandas(clean_chunk)\n\n        # Écriture dans le fichier Parquet\n        if writer is None:\n            writer = pq.ParquetWriter(parquet_path, table.schema, compression='snappy')\n        writer.write_table(table)\n    \n    # Fermer l'écrivain proprement\n    if writer:\n        writer.close()\n    \n    print(f\"Fichier sauvegardé : {parquet_path}\")\n\n# Traiter les deux fichiers CSV (training et testing)\nprocess_csv_in_chunks(\n    \"/kaggle/input/processed-yelp-dataset/data/review_training.csv\",\n    \"yelp_reviews_train_preprocessed.parquet\"\n)\n\nprocess_csv_in_chunks(\n    \"/kaggle/input/processed-yelp-dataset/data/review_testing.csv\",\n    \"yelp_reviews_test_preprocessed.parquet\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T12:21:49.213537Z","iopub.execute_input":"2025-11-19T12:21:49.214086Z","iopub.status.idle":"2025-11-19T12:24:09.420984Z","shell.execute_reply.started":"2025-11-19T12:21:49.214047Z","shell.execute_reply":"2025-11-19T12:24:09.419561Z"}},"outputs":[{"name":"stdout","text":"Fichier sauvegardé : yelp_reviews_train_preprocessed.parquet\nFichier sauvegardé : yelp_reviews_test_preprocessed.parquet\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\n\n# Charger le fichier Parquet prétraité\ntrain_reviews = pd.read_parquet(\"yelp_reviews_train_preprocessed.parquet\")\n\n# Nettoyage et filtrage\ndef clean_and_filter(df):\n    # Supprimer les doublons sur [user_id, business_id, stars]\n    df = df.drop_duplicates(subset=[\"user_id\", \"business_id\", \"stars\"])\n    \n    # Supprimer les lignes avec valeurs manquantes importantes\n    df = df.dropna(subset=[\"user_id\", \"business_id\", \"stars\", \"text\"])\n    \n    # Supprimer seulement les reviews trop courtes (< 10 caractères)\n    df = df[df['text'].str.len() >= 10]\n    \n    return df\n\ntrain_reviews = clean_and_filter(train_reviews)\n\n# Vérification\nprint(\"=== Vérification du train ===\")\nprint(\"Shape :\", train_reviews.shape)\nprint(\"Nombre de doublons sur [user_id, business_id, stars] :\", \n      train_reviews.duplicated(subset=[\"user_id\", \"business_id\", \"stars\"]).sum())\nprint(\"\\nValeurs manquantes par colonne :\")\nprint(train_reviews.isna().sum())\nprint(\"\\nLongueur des textes :\")\nprint(train_reviews['text'].str.len().describe())\nprint(\"\\nAperçu :\")\nprint(train_reviews.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T12:28:44.641282Z","iopub.execute_input":"2025-11-19T12:28:44.641742Z","iopub.status.idle":"2025-11-19T12:29:40.439666Z","shell.execute_reply.started":"2025-11-19T12:28:44.641713Z","shell.execute_reply":"2025-11-19T12:29:40.437627Z"}},"outputs":[{"name":"stdout","text":"=== Vérification du train ===\nShape : (4444812, 6)\nNombre de doublons sur [user_id, business_id, stars] : 0\n\nValeurs manquantes par colonne :\nreview_id      0\nuser_id        0\nbusiness_id    0\nstars          0\ntext           0\ndate           1\ndtype: int64\n\nLongueur des textes :\ncount    4.444812e+06\nmean     5.498948e+02\nstd      5.125874e+02\nmin      1.000000e+01\n25%      2.230000e+02\n50%      3.910000e+02\n75%      6.930000e+02\nmax      5.000000e+03\nName: text, dtype: float64\n\nAperçu :\n                review_id                 user_id             business_id  \\\n0  bwaSF4fYDBgyls0MBVcuOw  ZvmZM7LBzaYbpznpHlTANA  lmgCjE_h9VkpJYjsD2Li0A   \n1  hOl6MoudNRyeK7iC8V_UvA  UMS5obm9Gwr7AyfvEJyhkw  fGYnHzFr1z2kv7bPRW6VMA   \n2  VnVU0FtwvQtOHcB9unmVtg  9a0US1vaEBiQlMl2j2vmrg  7EhTT4iEuA7JaaWA-eI3Qw   \n3  9uJavmwzNEy10xmvd4935A  bKG5aNvEBU8Ckzf8JbXjlA  fGYnHzFr1z2kv7bPRW6VMA   \n4  TZ-WMedZ6YnYH1RpJF8CnA  bSAEDuyuE4-c3etP8_5lLA  TTsBa6e2qgIEkiFQfYLmYA   \n\n   stars                                               text  \\\n0      3  Why 3 stars?\\n\\nI love Target, but here is jus...   \n1      5  Top notch! He will cater our wedding and  Fran...   \n2      5  Best place to get a vietnamese sandwich and bo...   \n3      4  We came here a couple weeks ago. We got greete...   \n4      5  Best place for classic ice cream. Service is g...   \n\n                  date  \n0  2018-05-02 12:31:05  \n1  2014-10-02 11:32:33  \n2  2015-04-20 14:30:49  \n3  2018-09-18 04:28:44  \n4  2016-02-03 20:39:04  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n\n# Charger le fichier Parquet prétraité pour le test\ntest_reviews = pd.read_parquet(\"yelp_reviews_test_preprocessed.parquet\")\n\n# Nettoyage et filtrage (même fonction que pour le train)\ndef clean_and_filter(df):\n    # Supprimer les doublons sur [user_id, business_id, stars]\n    df = df.drop_duplicates(subset=[\"user_id\", \"business_id\", \"stars\"])\n    \n    # Supprimer les lignes avec valeurs manquantes importantes\n    df = df.dropna(subset=[\"user_id\", \"business_id\", \"stars\", \"text\"])\n    \n    # Supprimer seulement les reviews trop courtes (< 15 caractères)\n    df = df[df['text'].str.len() >= 13]\n    \n    return df\n\ntest_reviews = clean_and_filter(test_reviews)\n\n# Vérification\nprint(\"=== Vérification du test ===\")\nprint(\"Shape :\", test_reviews.shape)\nprint(\"Nombre de doublons sur [user_id, business_id, stars] :\", \n      test_reviews.duplicated(subset=[\"user_id\", \"business_id\", \"stars\"]).sum())\nprint(\"\\nValeurs manquantes par colonne :\")\nprint(test_reviews.isna().sum())\nprint(\"\\nLongueur des textes :\")\nprint(test_reviews['text'].str.len().describe())\nprint(\"\\nAperçu :\")\nprint(test_reviews.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T12:31:19.435794Z","iopub.execute_input":"2025-11-19T12:31:19.436325Z","iopub.status.idle":"2025-11-19T12:31:33.526937Z","shell.execute_reply.started":"2025-11-19T12:31:19.436283Z","shell.execute_reply":"2025-11-19T12:31:33.525700Z"}},"outputs":[{"name":"stdout","text":"=== Vérification du test ===\nShape : (1155276, 6)\nNombre de doublons sur [user_id, business_id, stars] : 0\n\nValeurs manquantes par colonne :\nreview_id      0\nuser_id        0\nbusiness_id    0\nstars          0\ntext           0\ndate           0\ndtype: int64\n\nLongueur des textes :\ncount    1.155276e+06\nmean     5.087396e+02\nstd      4.788763e+02\nmin      1.300000e+01\n25%      2.070000e+02\n50%      3.620000e+02\n75%      6.370000e+02\nmax      5.000000e+03\nName: text, dtype: float64\n\nAperçu :\n                review_id                 user_id             business_id  \\\n0  5-tLhwWjSzHYN6NyMy9Suw  IlWLPCRQp8iqX0X-ExQccQ  bdfZdB2MTXlT6-RBjSIpQg   \n1  4M72cXIgMLwXo8METuXZeQ  nfLL3vn_4-zXAfoh7hWiZg  MbzgGsMQpGyVrUJXi_Jw0Q   \n2  oC0ADnj65JKMk5zu6FDz5w  kV4eLnU5fm-ieJCEeqNiIQ  ZY2feSm2sc5nTXYHnKpMGQ   \n3  VpibERhdmOvSagzKSdVuSw  sWVVqJQAl6w6cOzbUsUC3A  tsx84z4c0B-y6J5fqfvBqg   \n4  1HmF9bqJvEFY0YJKv93Wsg  hflEbENcF1_TC2IoyX_Y4w  q8ZC6BgKh19RkLmH8bJ_Ig   \n\n   stars                                               text  \\\n0      4  The fried rice and pho are really delicious. T...   \n1      5  love this little spot on state street! I went ...   \n2      5  Not sure why they dont have more stars but I t...   \n3      5  Ty was amazing it was the first time and he ga...   \n4      5  They did an awesome job and price was very rea...   \n\n                  date  \n0  2020-01-26 21:21:04  \n1  2020-01-08 07:25:10  \n2  2020-01-06 02:31:08  \n3  2020-01-15 12:59:01  \n4  2020-01-04 00:51:40  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\n\n# Fonction de vérification\ndef check_issues(df, col='text'):\n    return {\n        'multi_space': df[col].str.contains(r'\\s{2,}').sum(),\n        'excessive_punct': df[col].str.contains(r'([!?.,])\\1+').sum(),\n        'leading_trailing_space': df[col].str.contains(r'^\\s+|\\s+$').sum(),\n        'uppercase_words': df[col].str.contains(r'[A-Z]').sum(),\n        'urls': df[col].str.contains(r'http\\S+|www\\S+').sum(),\n        'non_alphanumeric': df[col].str.contains(r'[^a-zA-Z0-9\\s!?.,]').sum(),\n        'digits': df[col].str.contains(r'\\d').sum()\n    }\n\n# Fonction de nettoyage\ndef clean_text(df, col='text'):\n    df[col] = (\n        df[col]\n        .str.strip()  # supprimer espaces début/fin\n        .str.lower()  # tout en minuscules\n        .str.replace(r'\\s{2,}', ' ', regex=True)  # réduire espaces multiples\n        .str.replace(r'([!?.,])\\1+', r'\\1', regex=True)  # réduire ponctuation répétée\n        .str.replace(r'http\\S+|www\\S+', '', regex=True)  # supprimer URL\n        .str.replace(r'[\\n\\t]+', ' ', regex=True)  # remplacer retours à la ligne/tab par espace\n        .str.replace(r'[^a-zA-Z0-9\\s!?.,]', '', regex=True)  # supprimer caractères spéciaux\n        .str.replace(r'\\d+', '', regex=True)  # supprimer chiffres\n    )\n    return df\n\n#  Nettoyage et vérification pour train_reviews \nprint(\"Train reviews avant nettoyage \")\nprint(check_issues(train_reviews))\n\ntrain_reviews = clean_text(train_reviews)\n\nprint(\" Train reviews après nettoyage\")\nprint(check_issues(train_reviews))\n\n# Nettoyage et vérification pour test_reviews \nprint(\"\\n Test reviews avant nettoyage \")\nprint(check_issues(test_reviews))\n\ntest_reviews = clean_text(test_reviews)\n\nprint(\"Test reviews après nettoyage\")\nprint(check_issues(test_reviews))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T12:58:20.170960Z","iopub.execute_input":"2025-11-19T12:58:20.173709Z","iopub.status.idle":"2025-11-19T13:15:03.441916Z","shell.execute_reply.started":"2025-11-19T12:58:20.173623Z","shell.execute_reply":"2025-11-19T13:15:03.439778Z"}},"outputs":[{"name":"stdout","text":"Train reviews avant nettoyage \n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_83/855551558.py:7: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  'excessive_punct': df[col].str.contains(r'([!?.,])\\1+').sum(),\n","output_type":"stream"},{"name":"stdout","text":"{'multi_space': 2223066, 'excessive_punct': 1073389, 'leading_trailing_space': 0, 'uppercase_words': 4424750, 'urls': 7158, 'non_alphanumeric': 3424494, 'digits': 1759539}\n Train reviews après nettoyage\n{'multi_space': 1825386, 'excessive_punct': 42420, 'leading_trailing_space': 83077, 'uppercase_words': 0, 'urls': 4, 'non_alphanumeric': 0, 'digits': 0}\n\n Test reviews avant nettoyage \n{'multi_space': 515229, 'excessive_punct': 262800, 'leading_trailing_space': 0, 'uppercase_words': 1148485, 'urls': 1627, 'non_alphanumeric': 871418, 'digits': 446181}\nTest reviews après nettoyage\n{'multi_space': 457884, 'excessive_punct': 8578, 'leading_trailing_space': 20326, 'uppercase_words': 0, 'urls': 1, 'non_alphanumeric': 0, 'digits': 0}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\n\n# Fonction de vérification finale\ndef check_issues(df, col='text'):\n    return {\n        'multi_space': df[col].str.contains(r'\\s{2,}').sum(),\n        'excessive_punct': df[col].str.contains(r'([!?.,])\\1+').sum(),  # plus d'une répétition\n        'leading_trailing_space': df[col].str.contains(r'^\\s+|\\s+$').sum(),\n        'urls': df[col].str.contains(r'http\\S+|www\\S+').sum()\n    }\n\n# Fonction de nettoyage final\ndef final_clean_text(df, col='text'):\n    df[col] = (\n        df[col]\n        .str.lower()                                 # tout en minuscules\n        .str.replace(r'\\s+', ' ', regex=True)       # réduire tous les espaces multiples, tabulations et retours à la ligne\n        .str.replace(r'([!?.,])\\1+', r'\\1', regex=True)  # ponctuation répétée → 1 seule occurrence\n        .str.replace(r'http\\S+|www\\S+', '', regex=True)  # supprimer toutes les URLs\n        .str.strip()                                 # supprimer espaces début/fin\n    )\n    return df\n\n#  Nettoyage et vérification pour train_reviews\nprint(\"Train reviews avant nettoyage final \")\nprint(check_issues(train_reviews))\n\ntrain_reviews = final_clean_text(train_reviews)\n\nprint(\"Train reviews après nettoyage final \")\nprint(check_issues(train_reviews))\n\n# Nettoyage et vérification pour test_reviews \nprint(\"\\n Test reviews avant nettoyage final\")\nprint(check_issues(test_reviews))\n\ntest_reviews = final_clean_text(test_reviews)\n\nprint(\" Test reviews après nettoyage final\")\nprint(check_issues(test_reviews))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:21:13.189732Z","iopub.execute_input":"2025-11-19T13:21:13.190368Z","iopub.status.idle":"2025-11-19T13:34:48.011052Z","shell.execute_reply.started":"2025-11-19T13:21:13.190326Z","shell.execute_reply":"2025-11-19T13:34:48.009211Z"}},"outputs":[{"name":"stdout","text":"Train reviews avant nettoyage final \n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_83/1486112820.py:7: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  'excessive_punct': df[col].str.contains(r'([!?.,])\\1+').sum(),  # plus d'une répétition\n","output_type":"stream"},{"name":"stdout","text":"{'multi_space': 1825386, 'excessive_punct': 42420, 'leading_trailing_space': 83077, 'urls': 4}\nTrain reviews après nettoyage final \n{'multi_space': 0, 'excessive_punct': 0, 'leading_trailing_space': 0, 'urls': 0}\n\n Test reviews avant nettoyage final\n{'multi_space': 457884, 'excessive_punct': 8578, 'leading_trailing_space': 20326, 'urls': 1}\n Test reviews après nettoyage final\n{'multi_space': 1, 'excessive_punct': 0, 'leading_trailing_space': 0, 'urls': 0}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"for df in [train_reviews, test_reviews]:\n    df['user_id'] = df['user_id'].astype('category')\n    df['business_id'] = df['business_id'].astype('category')\n    df['stars'] = df['stars'].astype('int8')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:36:13.845022Z","iopub.execute_input":"2025-11-19T13:36:13.846543Z","iopub.status.idle":"2025-11-19T13:36:28.092770Z","shell.execute_reply.started":"2025-11-19T13:36:13.846495Z","shell.execute_reply":"2025-11-19T13:36:28.091368Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:36:28.095278Z","iopub.execute_input":"2025-11-19T13:36:28.095614Z","iopub.status.idle":"2025-11-19T13:36:39.789587Z","shell.execute_reply.started":"2025-11-19T13:36:28.095589Z","shell.execute_reply":"2025-11-19T13:36:39.788363Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"418"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"all_reviews = pd.concat([train_reviews, test_reviews], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:36:39.791128Z","iopub.execute_input":"2025-11-19T13:36:39.792282Z","iopub.status.idle":"2025-11-19T13:36:46.701201Z","shell.execute_reply.started":"2025-11-19T13:36:39.792223Z","shell.execute_reply":"2025-11-19T13:36:46.698371Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\n\n#  Créer une copie du dataset pour ne pas modifier l'original \nall_reviews_copy = all_reviews.copy()\n\n# Nettoyage rapide \n# Supprimer les textes vides\nall_reviews_copy = all_reviews_copy[all_reviews_copy['text'].str.strip() != \"\"]\n# Supprimer les doublons sur user, business et note\nall_reviews_copy = all_reviews_copy.drop_duplicates(subset=[\"user_id\", \"business_id\", \"stars\"])\n\n# Supprimer les lignes avec valeurs manquantes importantes\nall_reviews_copy = all_reviews_copy.dropna(subset=[\"user_id\", \"business_id\", \"stars\", \"text\"])\n\n#  Fonction de filtrage itératif \ndef iterative_filter(df, min_user_reviews=5, min_business_reviews=5):\n    \"\"\"\n    Filtre le dataset pour garantir :\n    - Chaque utilisateur a au moins `min_user_reviews` reviews\n    - Chaque business a au moins `min_business_reviews` reviews\n    La fonction s'exécute de manière itérative jusqu'à stabilisation.\n    \"\"\"\n    while True:\n        # Comptage des reviews par user et par business\n        user_counts = df['user_id'].value_counts()\n        business_counts = df['business_id'].value_counts()\n        \n        # Filtrage\n        filtered_df = df[\n            df['user_id'].isin(user_counts[user_counts >= min_user_reviews].index) &\n            df['business_id'].isin(business_counts[business_counts >= min_business_reviews].index)\n        ]\n        \n        # Si pas de changement, sortir de la boucle\n        if len(filtered_df) == len(df):\n            break\n        df = filtered_df\n    \n    return df\n\n#  Appliquer le filtrage \nall_reviews_filtered = iterative_filter(\n    all_reviews_copy, \n    min_user_reviews=15, \n    min_business_reviews=15\n)\n\n# Vérifications finales \nprint(\"Shape finale :\", all_reviews_filtered.shape)\nprint(\"Utilisateurs uniques :\", all_reviews_filtered['user_id'].nunique())\nprint(\"Businesses uniques :\", all_reviews_filtered['business_id'].nunique())\nprint(\"Reviews totales :\", len(all_reviews_filtered))\nprint(\"Utilisateur avec le moins de reviews :\", all_reviews_filtered['user_id'].value_counts().min())\nprint(\"Business avec le moins de reviews :\", all_reviews_filtered['business_id'].value_counts().min())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:36:46.709455Z","iopub.execute_input":"2025-11-19T13:36:46.709980Z","iopub.status.idle":"2025-11-19T13:37:31.063233Z","shell.execute_reply.started":"2025-11-19T13:36:46.709948Z","shell.execute_reply":"2025-11-19T13:37:31.061952Z"}},"outputs":[{"name":"stdout","text":"Shape finale : (1148465, 6)\nUtilisateurs uniques : 31678\nBusinesses uniques : 23588\nReviews totales : 1148465\nUtilisateur avec le moins de reviews : 15\nBusiness avec le moins de reviews : 15\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(\"Colonnes disponibles :\", all_reviews_filtered.columns.tolist())\nnum_duplicates = all_reviews_filtered.duplicated(subset=[\"user_id\", \"business_id\", \"stars\"]).sum()\nprint(\"Nombre de doublons restants :\", num_duplicates)\nmissing_values = all_reviews_filtered[[\"user_id\", \"business_id\", \"stars\", \"text\"]].isna().sum()\nprint(\"Valeurs manquantes par colonne :\\n\", missing_values)\nempty_texts = (all_reviews_filtered['text'].str.strip() == \"\").sum()\nprint(\"Nombre de textes vides :\", empty_texts)\nprint(\"Nombre minimal de reviews par utilisateur :\", all_reviews_filtered['user_id'].value_counts().min())\nprint(\"Nombre minimal de reviews par business :\", all_reviews_filtered['business_id'].value_counts().min())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:37:31.064579Z","iopub.execute_input":"2025-11-19T13:37:31.064875Z","iopub.status.idle":"2025-11-19T13:37:33.753275Z","shell.execute_reply.started":"2025-11-19T13:37:31.064851Z","shell.execute_reply":"2025-11-19T13:37:33.751891Z"}},"outputs":[{"name":"stdout","text":"Colonnes disponibles : ['review_id', 'user_id', 'business_id', 'stars', 'text', 'date']\nNombre de doublons restants : 0\nValeurs manquantes par colonne :\n user_id        0\nbusiness_id    0\nstars          0\ntext           0\ndtype: int64\nNombre de textes vides : 0\nNombre minimal de reviews par utilisateur : 15\nNombre minimal de reviews par business : 15\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# # Exporter le dataframe en CSV\n# all_reviews_filtered.to_csv('all_reviews_filtered.v2.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport zipfile\n\n# Chemin du CSV à sauvegarder\ncsv_path = \"/kaggle/working/all_reviews_filtered.v2.csv\"\nall_reviews_filtered.to_csv(csv_path, index=False)\n\n# Créer le ZIP\nzip_path = \"/kaggle/working/all_reviews_filtered.v2.zip\"\nwith zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n    zipf.write(csv_path, arcname=\"all_reviews_filtered.v2.csv\")  # Nom à l'intérieur du ZIP\n\nprint(f\"Fichier ZIP créé : {zip_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:37:33.755426Z","iopub.execute_input":"2025-11-19T13:37:33.755710Z","iopub.status.idle":"2025-11-19T13:39:20.711469Z","shell.execute_reply.started":"2025-11-19T13:37:33.755691Z","shell.execute_reply":"2025-11-19T13:39:20.710169Z"}},"outputs":[{"name":"stdout","text":"Fichier ZIP créé : /kaggle/working/all_reviews_filtered.v2.zip\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import re\n\n# Fonction pour analyser les problèmes dans les textes\ndef analyze_text_issues(df, col='text', long_word_threshold=30):\n    return {\n        'multi_space': df[col].str.contains(r'\\s{2,}').sum(),                          # espaces multiples\n        'excessive_punct': df[col].str.contains(r'([!?.,])\\1+').sum(),                 # ponctuation répétée\n        'long_words': df[col].str.split().apply(\n            lambda words: any(len(w) > long_word_threshold for w in words)\n        ).sum(),                                                                      # mots trop longs\n        'emojis_or_special_chars': df[col].str.contains(r'[^\\w\\s,\\.!?]', regex=True).sum(), # emojis/caractères spéciaux\n        'urls': df[col].str.contains(r'http\\S+|www\\S+').sum(),                         # URLs\n        'html_tags': df[col].str.contains(r'<[^>]+>').sum()                             # balises HTML\n    }\n\n# Résumé des problèmes\nissues_summary = analyze_text_issues(all_reviews_filtered)\n\nprint(\" Résumé des problèmes dans les textes\")\nfor issue, count in issues_summary.items():\n    print(f\"{issue}: {count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\n# Fonction pour détecter les mots répétés successivement\ndef find_repeated_words(df, col='text'):\n    repeated_word_examples = []\n    pattern = re.compile(r'\\b(\\w+)\\b(?:\\s+\\1\\b)+', flags=re.IGNORECASE)  # capture répétitions successives\n    for idx, text in df[col].items():  # <-- utiliser items() au lieu de iteritems()\n        matches = pattern.findall(text)\n        if matches:\n            repeated_word_examples.append((idx, matches))\n    return repeated_word_examples\n\n# Récupérer les exemples\nrepeated_words_found = find_repeated_words(all_reviews_filtered)\n\n# Afficher quelques exemples\nprint(f\"Nombre de lignes avec mots répétés successivement :\", len(repeated_words_found))\nprint(\"Exemples :\")\nfor idx, words in repeated_words_found[:5]:  # Affiche 5 premiers exemples\n    print(f\"Ligne {idx} : {words}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\n# Fonction pour supprimer les répétitions consécutives de mots\ndef clean_repeated_words(df, col='text'):\n    \"\"\"\n    Supprime les répétitions consécutives de mots dans tout le dataset.\n    Exemple :\n    \"good good service\" → \"good service\"\n    \"I I love this\" → \"I love this\"\n    \"\"\"\n    pattern = re.compile(r'\\b(\\w+)(\\s+\\1\\b)+', flags=re.IGNORECASE)\n    df[col] = df[col].str.replace(pattern, r'\\1', regex=True)\n    return df\n\n# Appliquer sur tout le dataset\nall_reviews_filtered = clean_repeated_words(all_reviews_filtered, col='text')\n\n# Vérification globale\nprint(\"Nettoyage des mots répétés terminé \")\nprint(\"Nombre total de lignes :\", len(all_reviews_filtered))\nprint(\"Exemple aléatoire :\")\nprint(all_reviews_filtered.sample(1)['text'].values[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\n# Motif pour détecter les mots répétés consécutifs\npattern = re.compile(r'\\b(\\w+)(\\s+\\1\\b)+', flags=re.IGNORECASE)\n\n# Vérifier si des répétitions existent encore\nrepeated_rows = all_reviews_filtered[all_reviews_filtered['text'].str.contains(pattern, na=False)]\n\nif len(repeated_rows) == 0:\n    print(\"Aucune répétition consécutive détectée. Tous les textes sont nettoyés.\")\nelse:\n    print(f\"Il reste {len(repeated_rows)} lignes avec des répétitions consécutives.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport zipfile\n\n# Sauvegarder le dataset filtré dans le dossier de travail Kaggle\ncsv_path = \"/kaggle/working/all_reviews_filtered.csv\"\nall_reviews_filtered.to_csv(csv_path, index=False)\n\n# Créer le ZIP\nzip_path = \"/kaggle/working/all_reviews_filtered.zip\"\nwith zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n    zipf.write(csv_path, arcname=\"all_reviews_filtered.csv\")  # arcname définit le nom à l'intérieur du ZIP\n\nprint(f\"Fichier ZIP créé : {zip_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}